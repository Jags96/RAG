{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jba515tLfbg",
        "outputId": "dbd28360-719c-4338-80a3-ef36babd9549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
            "  Downloading thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
            "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)\n",
            "Requirement already satisfied: jinja2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy) (72.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy) (24.1)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy) (2.1.0)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.23.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.23.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.7 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.0-cp311-cp311-macosx_11_0_arm64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
            "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
            "Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
            "Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
            "Downloading pydantic_core-2.23.3-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
            "Downloading thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
            "Downloading marisa_trie-1.2.0-cp311-cp311-macosx_11_0_arm64.whl (174 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, tqdm, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.0\n",
            "    Uninstalling numpy-2.1.0:\n",
            "      Successfully uninstalled numpy-2.1.0\n",
            "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-1.26.4 preshed-3.0.9 pydantic-2.9.1 pydantic-core-2.23.3 rich-13.8.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.8.0 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 tqdm-4.66.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting spacy<3.8.0,>=3.7.2 (from en-core-web-sm==3.7.1)\n",
            "  Downloading spacy-3.7.6.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (72.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/jagathkumarreddyk/miniconda3/envs/dsprojects/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "Building wheels for collected packages: spacy\n",
            "  Building wheel for spacy (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for spacy: filename=spacy-3.7.6-cp311-cp311-macosx_11_0_arm64.whl size=5912861 sha256=ec4656c1290ad8d92c6685bd5bc69e50f45bc0888cb3f442f44cd5484187cd65\n",
            "  Stored in directory: /Users/jagathkumarreddyk/Library/Caches/pip/wheels/d4/20/60/0719a53a3a30f17c67a2beb43d9b4a4cc30751716d1f4a2416\n",
            "Successfully built spacy\n",
            "Installing collected packages: spacy, en-core-web-sm\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.0\n",
            "    Uninstalling spacy-3.8.0:\n",
            "      Successfully uninstalled spacy-3.8.0\n",
            "Successfully installed en-core-web-sm-3.7.1 spacy-3.7.6\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vdmql3SLSqA"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "stopwords = list(STOP_WORDS)\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DNer5PsMzat"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It aims to enable computers to understand, interpret, and generate human language in a meaningful way.\n",
        "NLP encompasses a variety of tasks, including:\n",
        "Text mining: Extracting information from unstructured text data.\n",
        "Machine translation: Translating text from one language to another.\n",
        "Sentiment analysis: Determining the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
        "Information retrieval: Finding relevant information from a large dataset.\n",
        "Question answering: Answering questions posed in natural language.\n",
        "Chatbots and virtual assistants: Creating conversational agents that can interact with humans.\n",
        "To achieve these tasks, NLP techniques often involve:\n",
        "\n",
        "Tokenization: Breaking text into individual words or tokens.\n",
        "Part-of-speech tagging: Identifying the grammatical category of each word (noun, verb, etc.).\n",
        "Named entity recognition: Identifying named entities like people, organizations, and locations.\n",
        "Dependency parsing: Analyzing the grammatical structure of a sentence.\n",
        "Machine learning: Using algorithms to train models on large datasets.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUASrE72LseV",
        "outputId": "7082abd4-a04a-4fd3-804c-b3d0b74e4ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', 'It', 'aims', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'meaningful', 'way', '.', '\\n', 'NLP', 'encompasses', 'a', 'variety', 'of', 'tasks', ',', 'including', ':', '\\n', 'Text', 'mining', ':', 'Extracting', 'information', 'from', 'unstructured', 'text', 'data', '.', '\\n', 'Machine', 'translation', ':', 'Translating', 'text', 'from', 'one', 'language', 'to', 'another', '.', '\\n', 'Sentiment', 'analysis', ':', 'Determining', 'the', 'sentiment', '(', 'positive', ',', 'negative', ',', 'or', 'neutral', ')', 'expressed', 'in', 'a', 'piece', 'of', 'text', '.', '\\n', 'Information', 'retrieval', ':', 'Finding', 'relevant', 'information', 'from', 'a', 'large', 'dataset', '.', '\\n', 'Question', 'answering', ':', 'Answering', 'questions', 'posed', 'in', 'natural', 'language', '.', '\\n', 'Chatbots', 'and', 'virtual', 'assistants', ':', 'Creating', 'conversational', 'agents', 'that', 'can', 'interact', 'with', 'humans', '.', '\\n', 'To', 'achieve', 'these', 'tasks', ',', 'NLP', 'techniques', 'often', 'involve', ':', '\\n\\n', 'Tokenization', ':', 'Breaking', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.', '\\n', 'Part', '-', 'of', '-', 'speech', 'tagging', ':', 'Identifying', 'the', 'grammatical', 'category', 'of', 'each', 'word', '(', 'noun', ',', 'verb', ',', 'etc', '.', ')', '.', '\\n', 'Named', 'entity', 'recognition', ':', 'Identifying', 'named', 'entities', 'like', 'people', ',', 'organizations', ',', 'and', 'locations', '.', '\\n', 'Dependency', 'parsing', ':', 'Analyzing', 'the', 'grammatical', 'structure', 'of', 'a', 'sentence', '.', '\\n', 'Machine', 'learning', ':', 'Using', 'algorithms', 'to', 'train', 'models', 'on', 'large', 'datasets', '.']\n",
            "{'Natural': 1, 'Language': 1, 'Processing': 1, 'NLP': 3, 'field': 1, 'artificial': 1, 'intelligence': 1, 'focuses': 1, 'interaction': 1, 'computers': 2, 'human': 2, 'language': 4, 'aims': 1, 'enable': 1, 'understand': 1, 'interpret': 1, 'generate': 1, 'meaningful': 1, 'way': 1, 'encompasses': 1, 'variety': 1, 'tasks': 2, 'including': 1, 'Text': 1, 'mining': 1, 'Extracting': 1, 'information': 2, 'unstructured': 1, 'text': 4, 'data': 1, 'Machine': 2, 'translation': 1, 'Translating': 1, 'Sentiment': 1, 'analysis': 1, 'Determining': 1, 'sentiment': 1, 'positive': 1, 'negative': 1, 'neutral': 1, 'expressed': 1, 'piece': 1, 'Information': 1, 'retrieval': 1, 'Finding': 1, 'relevant': 1, 'large': 2, 'dataset': 1, 'Question': 1, 'answering': 1, 'Answering': 1, 'questions': 1, 'posed': 1, 'natural': 1, 'Chatbots': 1, 'virtual': 1, 'assistants': 1, 'Creating': 1, 'conversational': 1, 'agents': 1, 'interact': 1, 'humans': 1, 'achieve': 1, 'techniques': 1, 'involve': 1, '\\n\\n': 1, 'Tokenization': 1, 'Breaking': 1, 'individual': 1, 'words': 1, 'tokens': 1, 'speech': 1, 'tagging': 1, 'Identifying': 2, 'grammatical': 2, 'category': 1, 'word': 1, 'noun': 1, 'verb': 1, 'etc': 1, 'Named': 1, 'entity': 1, 'recognition': 1, 'named': 1, 'entities': 1, 'like': 1, 'people': 1, 'organizations': 1, 'locations': 1, 'Dependency': 1, 'parsing': 1, 'Analyzing': 1, 'structure': 1, 'sentence': 1, 'learning': 1, 'algorithms': 1, 'train': 1, 'models': 1, 'datasets': 1}\n"
          ]
        }
      ],
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "punctuation = punctuation + '\\n'\n",
        "punctuation\n",
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1\n",
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI7AltRYLs5K",
        "outputId": "e4d056e2-f3f4-42e4-c718-92ecf3c8110f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Natural': 0.25, 'Language': 0.25, 'Processing': 0.25, 'NLP': 0.75, 'field': 0.25, 'artificial': 0.25, 'intelligence': 0.25, 'focuses': 0.25, 'interaction': 0.25, 'computers': 0.5, 'human': 0.5, 'language': 1.0, 'aims': 0.25, 'enable': 0.25, 'understand': 0.25, 'interpret': 0.25, 'generate': 0.25, 'meaningful': 0.25, 'way': 0.25, 'encompasses': 0.25, 'variety': 0.25, 'tasks': 0.5, 'including': 0.25, 'Text': 0.25, 'mining': 0.25, 'Extracting': 0.25, 'information': 0.5, 'unstructured': 0.25, 'text': 1.0, 'data': 0.25, 'Machine': 0.5, 'translation': 0.25, 'Translating': 0.25, 'Sentiment': 0.25, 'analysis': 0.25, 'Determining': 0.25, 'sentiment': 0.25, 'positive': 0.25, 'negative': 0.25, 'neutral': 0.25, 'expressed': 0.25, 'piece': 0.25, 'Information': 0.25, 'retrieval': 0.25, 'Finding': 0.25, 'relevant': 0.25, 'large': 0.5, 'dataset': 0.25, 'Question': 0.25, 'answering': 0.25, 'Answering': 0.25, 'questions': 0.25, 'posed': 0.25, 'natural': 0.25, 'Chatbots': 0.25, 'virtual': 0.25, 'assistants': 0.25, 'Creating': 0.25, 'conversational': 0.25, 'agents': 0.25, 'interact': 0.25, 'humans': 0.25, 'achieve': 0.25, 'techniques': 0.25, 'involve': 0.25, '\\n\\n': 0.25, 'Tokenization': 0.25, 'Breaking': 0.25, 'individual': 0.25, 'words': 0.25, 'tokens': 0.25, 'speech': 0.25, 'tagging': 0.25, 'Identifying': 0.5, 'grammatical': 0.5, 'category': 0.25, 'word': 0.25, 'noun': 0.25, 'verb': 0.25, 'etc': 0.25, 'Named': 0.25, 'entity': 0.25, 'recognition': 0.25, 'named': 0.25, 'entities': 0.25, 'like': 0.25, 'people': 0.25, 'organizations': 0.25, 'locations': 0.25, 'Dependency': 0.25, 'parsing': 0.25, 'Analyzing': 0.25, 'structure': 0.25, 'sentence': 0.25, 'learning': 0.25, 'algorithms': 0.25, 'train': 0.25, 'models': 0.25, 'datasets': 0.25}\n",
            "[Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language., It aims to enable computers to understand, interpret, and generate human language in a meaningful way.\n",
            ", NLP encompasses a variety of tasks, including:\n",
            "Text mining: Extracting information from unstructured text data.\n",
            ", Machine translation: Translating text from one language to another.\n",
            ", Sentiment analysis: Determining the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
            ", Information retrieval: Finding relevant information from a large dataset.\n",
            ", Question answering: Answering questions posed in natural language.\n",
            ", Chatbots and virtual assistants: Creating conversational agents that can interact with humans.\n",
            ", To achieve these tasks, NLP techniques often involve:\n",
            "\n",
            "Tokenization: Breaking text into individual words or tokens.\n",
            ", Part-of-speech tagging: Identifying the grammatical category of each word (noun, verb, etc.).\n",
            ", Named entity recognition: Identifying named entities like people, organizations, and locations.\n",
            ", Dependency parsing: Analyzing the grammatical structure of a sentence.\n",
            ", Machine learning: Using algorithms to train models on large datasets.]\n"
          ]
        }
      ],
      "source": [
        "max_frequency = max(word_frequencies.values())\n",
        "max_frequency\n",
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency\n",
        "print(word_frequencies)\n",
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvQgyRrLtG0",
        "outputId": "333d86a1-d635-486f-98d5-683a5e921783"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language.: 4.5,\n",
              " It aims to enable computers to understand, interpret, and generate human language in a meaningful way.: 3.75,\n",
              " NLP encompasses a variety of tasks, including:\n",
              " Text mining: Extracting information from unstructured text data.: 4.5,\n",
              " Machine translation: Translating text from one language to another.: 2.25,\n",
              " Sentiment analysis: Determining the sentiment (positive, negative, or neutral) expressed in a piece of text.: 3.0,\n",
              " Information retrieval: Finding relevant information from a large dataset.: 2.25,\n",
              " Question answering: Answering questions posed in natural language.: 2.25,\n",
              " Chatbots and virtual assistants: Creating conversational agents that can interact with humans.: 1.5,\n",
              " To achieve these tasks, NLP techniques often involve:\n",
              " \n",
              " Tokenization: Breaking text into individual words or tokens.: 3.25,\n",
              " Part-of-speech tagging: Identifying the grammatical category of each word (noun, verb, etc.).: 2.25,\n",
              " Named entity recognition: Identifying named entities like people, organizations, and locations.: 2.25,\n",
              " Dependency parsing: Analyzing the grammatical structure of a sentence.: 1.25,\n",
              " Machine learning: Using algorithms to train models on large datasets.: 1.75}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "sentence_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot5tkgufMFHH"
      },
      "outputs": [],
      "source": [
        "from heapq import nlargest\n",
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length\n",
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "summary\n",
        "final_summary = [word.text for word in summary]\n",
        "summary = ' '.join(final_summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6JmawZbMF9W",
        "outputId": "da884209-3831-42b0-97aa-6d50ae404b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "          \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language.\",\n",
            "          \"NLP encompasses a variety of tasks, including:\\nText mining: Extracting information from unstructured text data.\\n\",\n",
            "          \"It aims to enable computers to understand, interpret, and generate human language in a meaningful way.\\n\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "print(json.dumps(final_summary, indent=10))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
